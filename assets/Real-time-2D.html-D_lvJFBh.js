import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as p,b as i,a as l,e as s,d as t,w as a,r as d,o as k}from"./app-CSzeelyP.js";const g="/devopsme/assets/adh-ui-Capo3slK.png",c={},u={id:"adh-2d-实时数字人",tabindex:"-1"},m={class:"header-anchor",href:"#adh-2d-实时数字人"};function y(A,e){const n=d("highlight-orange"),h=d("BiliBili"),r=d("highlight-purple");return k(),p("div",null,[e[36]||(e[36]=i("p",null,"LLM 开源社区现有状态，非常活跃，大家开发 AI 原生应用时，可以参考选择。请以最新数据为准。",-1)),e[37]||(e[37]=i("p",null,"带着兴趣和疑问来听这个分享。分享完后，期待能够动手实践。",-1)),e[38]||(e[38]=i("h2",{id:"开源-lm",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#开源-lm"},[i("span",null,"开源 LM")])],-1)),e[39]||(e[39]=i("h3",{id:"国内",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#国内"},[i("span",null,"国内")])],-1)),i("ul",null,[i("li",null,[e[1]||(e[1]=t("阿里云 ")),s(n,null,{default:a(()=>e[0]||(e[0]=[t("Qwen3")])),_:1,__:[0]}),e[2]||(e[2]=t("，多模态 Qwen 2.5 VL, Qwen2.5-Omni"))]),i("li",null,[s(n,null,{default:a(()=>e[3]||(e[3]=[t("DeepSeek-R1(0528)")])),_:1,__:[3]}),e[4]||(e[4]=t(", DeepSeek-V3, 多模态目前还未有"))]),e[5]||(e[5]=i("li",null,"智谱 AI GLM-4，多模态 VisualGLM-6B, CogVLM-17B",-1)),e[6]||(e[6]=i("li",null,"腾讯云 Hunyuan-MoE-A52B，多模态 Hunyuan-DiT，Hunyuan3D 2.0",-1))]),e[40]||(e[40]=l('<h3 id="国外" tabindex="-1"><a class="header-anchor" href="#国外"><span>国外</span></a></h3><ul><li>Meta Llama4, 多模态 Llama4, Llama 4 Scout, Llama 4 Maverick</li><li>Mistral Small 3.1, 多模态 Mistral Small 3.1</li><li>Google Gemma3, 多模态 Gemma3</li><li>xAI Grok-1, Grok-2, 多模态目前还未有</li></ul><h2 id="开源-lm-发布平台" tabindex="-1"><a class="header-anchor" href="#开源-lm-发布平台"><span>开源 LM 发布平台</span></a></h2><ul><li><a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">Hugging Face Hub</a></li><li><a href="https://www.modelscope.cn/models" target="_blank" rel="noopener noreferrer">ModelScope</a></li><li><a href="https://ollama.com/search" target="_blank" rel="noopener noreferrer">Ollama</a></li></ul><h2 id="开源-lm-部署工具" tabindex="-1"><a class="header-anchor" href="#开源-lm-部署工具"><span>开源 LM 部署工具</span></a></h2><table><thead><tr><th style="text-align:left;">主流 LM 部署工具</th><th style="text-align:left;">其它 LM 部署工具</th></tr></thead><tbody><tr><td style="text-align:left;">- <a href="https://github.com/ollama/ollama" target="_blank" rel="noopener noreferrer">Ollama</a></td><td style="text-align:left;">- <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener noreferrer">llama.cpp</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a></td><td style="text-align:left;">- <a href="https://github.com/Mozilla-Ocho/llamafile" target="_blank" rel="noopener noreferrer">llamafile</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">Hugging Face Transformers</a></td><td style="text-align:left;">- <a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener noreferrer">GPT4All</a></td></tr><tr><td style="text-align:left;"></td><td style="text-align:left;">- <a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a></td></tr><tr><td style="text-align:left;"></td><td style="text-align:left;">- <a href="https://github.com/mlc-ai/web-llm" target="_blank" rel="noopener noreferrer">WebLLM</a></td></tr><tr><td style="text-align:left;"></td><td style="text-align:left;">- <a href="https://github.com/ModelTC/lightllm" target="_blank" rel="noopener noreferrer">LightLLM</a></td></tr><tr><td style="text-align:left;"></td><td style="text-align:left;">- <a href="https://github.com/microsoft/BitNet" target="_blank" rel="noopener noreferrer">BitNet</a></td></tr></tbody></table><h2 id="开源-ai-agent-开发框架" tabindex="-1"><a class="header-anchor" href="#开源-ai-agent-开发框架"><span>开源 AI Agent 开发框架</span></a></h2><blockquote><p>2024 年 AI Agent 开发领域以开源社区为主，2025 年大厂开始发力，在我看来，AI Agent 开发框架会以开源社区为主。<br><a href="https://learn.microsoft.com/en-us/azure/ai-foundry/agents/overview" target="_blank" rel="noopener noreferrer">Azure AI Foundry Agent Service</a> 是一个将模型、工具、框架和治理机制整合为统一平台的系统，用于构建智能体。</p></blockquote><table><thead><tr><th style="text-align:left;">通⽤ AI Agent 开发框架</th><th style="text-align:left;">RAG 开发框架</th><th style="text-align:left;">低代码开发⼯具</th><th style="text-align:left;">类 Manus 开发框架</th></tr></thead><tbody><tr><td style="text-align:left;">- <a href="https://github.com/langchain-ai/langgraph" target="_blank" rel="noopener noreferrer">LangChain + LangGraph</a></td><td style="text-align:left;">- <a href="https://github.com/run-llama/llama_index" target="_blank" rel="noopener noreferrer">LlamaIndex</a></td><td style="text-align:left;">- <a href="https://github.com/langgenius/dify" target="_blank" rel="noopener noreferrer">Dify</a></td><td style="text-align:left;">- <a href="https://github.com/FoundationAgents/OpenManus" target="_blank" rel="noopener noreferrer">OpenManus</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/crewAIInc/crewAI" target="_blank" rel="noopener noreferrer">CrewAI</a></td><td style="text-align:left;">- <a href="https://github.com/microsoft/graphrag" target="_blank" rel="noopener noreferrer">GraphRAG</a></td><td style="text-align:left;">- <a href="https://github.com/labring/FastGPT" target="_blank" rel="noopener noreferrer">FastGPT</a></td><td style="text-align:left;">- <a href="https://github.com/camel-ai/owl" target="_blank" rel="noopener noreferrer">OWL</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/openai/openai-agents-python" target="_blank" rel="noopener noreferrer">Agent SDK</a></td><td style="text-align:left;">- <a href="https://github.com/chatchat-space/Langchain-Chatchat" target="_blank" rel="noopener noreferrer">LangChain-Chatchat</a></td><td style="text-align:left;">- <a href="https://github.com/FlowiseAI/Flowise" target="_blank" rel="noopener noreferrer">Flowise</a></td><td style="text-align:left;">- <a href="https://github.com/femto/minion-agent" target="_blank" rel="noopener noreferrer">Minion-Agent</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/Significant-Gravitas/AutoGPT" target="_blank" rel="noopener noreferrer">AutoGPT Platform</a></td><td style="text-align:left;"></td><td style="text-align:left;">- <a href="https://github.com/n8n-io/n8n" target="_blank" rel="noopener noreferrer">n8n</a></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer">AutoGen</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/FoundationAgents/MetaGPT" target="_blank" rel="noopener noreferrer">MetaGPT</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/camel-ai/camel" target="_blank" rel="noopener noreferrer">Camel</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/yoheinakajima/babyagi" target="_blank" rel="noopener noreferrer">BabyAGI</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/Link-AGI/AutoAgents" target="_blank" rel="noopener noreferrer">AutoAgents</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://google.github.io/adk-docs/" target="_blank" rel="noopener noreferrer">Google- ADK</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://strandsagents.com/latest/" target="_blank" rel="noopener noreferrer">AWS - Strands Agents</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr></tbody></table><h2 id="开源数字人项目" tabindex="-1"><a class="header-anchor" href="#开源数字人项目"><span>开源数字人项目</span></a></h2><table><thead><tr><th style="text-align:left;">⾮实时数字⼈</th><th style="text-align:left;">2D 实时数字人</th><th style="text-align:left;">3D 实时数字人</th></tr></thead><tbody><tr><td style="text-align:left;">- <a href="https://github.com/antgroup/echomimic_v2" target="_blank" rel="noopener noreferrer">EchoMimic V2</a></td><td style="text-align:left;">- <a href="https://github.com/wan-h/awesome-digital-human-live2d" target="_blank" rel="noopener noreferrer">ADH</a></td><td style="text-align:left;">- <a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat" target="_blank" rel="noopener noreferrer">阿里达摩院 OpenAvatarChat</a><br><a href="https://www.openavatarchat.ai/playground" target="_blank" rel="noopener noreferrer">Playground</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/duixcom/Duix.Heygem" target="_blank" rel="noopener noreferrer">Heygem</a></td><td style="text-align:left;">- <a href="https://github.com/lipku/livetalking" target="_blank" rel="noopener noreferrer">LiveTalking</a></td><td style="text-align:left;">- <a href="https://github.com/Henry-23/VideoChat" target="_blank" rel="noopener noreferrer">VideoChat</a></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/jixiaozhong/Sonic" target="_blank" rel="noopener noreferrer">Sonic</a></td><td style="text-align:left;">- <a href="https://github.com/anliyuan/Ultralight-Digital-Human" target="_blank" rel="noopener noreferrer">Ultralight-Digital-Human</a><br>(2.5D - AI 模型实时生成)</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/KwaiVGI/LivePortrait" target="_blank" rel="noopener noreferrer">LivePortrait</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/xszyou/fay" target="_blank" rel="noopener noreferrer">Fay</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/OpenTalker/SadTalker" target="_blank" rel="noopener noreferrer">SadTalker</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/bytedance/LatentSync" target="_blank" rel="noopener noreferrer">LatentSync</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;">- <a href="https://github.com/JOY-MM/JoyGen" target="_blank" rel="noopener noreferrer">JoyGen</a></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr></tbody></table><h2 id="商业-3d-数字人" tabindex="-1"><a class="header-anchor" href="#商业-3d-数字人"><span>商业 3D 数字人</span></a></h2>',12)),i("ul",null,[i("li",null,[e[8]||(e[8]=i("p",null,[i("a",{href:"https://blogs.nvidia.cn/blog/digital-humans-ace-generative-ai-microservices/",target:"_blank",rel:"noopener noreferrer"},"Nvidia ACE")],-1)),s(n,null,{default:a(()=>e[7]||(e[7]=[t("Nvidia ACE")])),_:1,__:[7]}),e[9]||(e[9]=t(" 是目前最为逼真的数字人服务，支持实时交互。当然，价格不菲。视频中展示了非常多的应用场景，还有 ")),e[10]||(e[10]=i("a",{href:"https://build.nvidia.com/nvidia/digital-humans-for-customer-service",target:"_blank",rel:"noopener noreferrer"},"在线 demo",-1)),e[11]||(e[11]=t(" 可以体验。")),e[12]||(e[12]=i("p",null,[i("a",{href:"https://www.bilibili.com/video/BV1Dz42187yB/",target:"_blank",rel:"noopener noreferrer"},"B 站上更完整的 Nvidia 数字人演示视频")],-1)),s(h,{bvid:"BV1Dz42187yB"})]),e[13]||(e[13]=i("li",null,[i("p",null,[i("a",{href:"https://www.metahuman.com/en-US",target:"_blank",rel:"noopener noreferrer"},"Metahuman"),t("。创建并驱动完全定制的高度逼真 3D 数字人。")])],-1))]),i("h2",u,[i("a",m,[i("span",null,[s(n,null,{default:a(()=>e[14]||(e[14]=[t("ADH 2D 实时数字人")])),_:1,__:[14]})])])]),i("p",null,[e[17]||(e[17]=t("开源项目 ")),e[18]||(e[18]=i("a",{href:"https://github.com/wan-h/awesome-digital-human-live2d",target:"_blank",rel:"noopener noreferrer"},"awesome-digital-human-live2d",-1)),e[19]||(e[19]=t("，缩写为 ADH，是国内优秀开发者")),s(n,null,{default:a(()=>e[15]||(e[15]=[t("一力辉")])),_:1,__:[15]}),e[20]||(e[20]=t("老师在 2024 年开发的。ADH 项目分成后端和前端两部分。")),s(n,null,{default:a(()=>e[16]||(e[16]=[t("后端使用 Python 开发，开发框架是 FastAPI。前端使用 TypeScript 开发，开发框架是 Next.js + React + HeroUI")])),_:1,__:[16]}),e[21]||(e[21]=t("。可以本地部署开源大模型，也可以使用")),e[22]||(e[22]=i("a",{href:"https://bailian.console.aliyun.com/?tab=model#/model-market",target:"_blank",rel:"noopener noreferrer"},"阿里云百炼模型广场",-1)),e[23]||(e[23]=t("。"))]),e[41]||(e[41]=i("h3",{id:"部署-adh-前后端",tabindex:"-1"},[i("a",{class:"header-anchor",href:"#部署-adh-前后端"},[i("span",null,"部署 ADH 前后端")])],-1)),s(r,null,{default:a(()=>e[24]||(e[24]=[t("后端 (Ubuntu 22.04/24.04):")])),_:1,__:[24]}),e[42]||(e[42]=l(`<p>ADH 后端就是一个基于 FastAPI 开发的服务，它的主要作用是集成外部的一些基础服务（包括 ASR、TTS、LLM 三类）和外部的 AI Agent，暴露 RESTful API 给前端使用。ADH 前端遵循 BFF 架构模式（Backend for Frontend）。而支持 BFF 架构前端的理想选择，就是微服务，主要是基于 RESTful API 的微服务。而使用 Python 开发微服务的首选开发框架就是 FastAPI。ADH 后端的代码采用标准的 OOP 面向对象风格编写</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Python 3.10 on 22.04 and Python 3.12 on 24.04 by default</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sudo</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> apt</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python3</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python3-pip</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python3-dev</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">curl</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -LsSf</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://astral.sh/uv/install.sh</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sh</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># FFmpeg</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sudo</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> apt</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> update</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">sudo</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> apt</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ffmpeg</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Clone ADH</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ~</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">git</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> clone</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://github.com/wan-h/awesome-digital-human-live2d.git</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> awesome-digital-human-live2d</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Init a uv project</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> init</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --python</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3.10</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Update \`pyproject.toml\` to use Tsinghua University&#39;s PyPI mirror</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Add the following section at the beginning of the file:</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># [[tool.uv.index]]</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># url = &quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># default = true</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Install dependencies</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> add</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> $(</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">cat</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> requirements.txt</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Create the main config file (port: 8002)</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> configs</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">cp</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> config_template.yaml</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> config.yaml</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Run the ADH backend server</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ~/awesome-digital-human-live2d</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> python</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> main.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2)),s(r,null,{default:a(()=>e[25]||(e[25]=[t("前端 (MacOS):")])),_:1,__:[25]}),e[43]||(e[43]=l(`<p>ADH 前端是一个标准的 H5 页面，而且支持被内嵌在其他 H5 页面中。Next.js 应用的架构遵循标准的 BFF（Backend for Frontend）架构模式。</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Install Node.js(v22.16.0) https://nodejs.org/en/download</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">npm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -g</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> next</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">npm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -g</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> heroui-cli</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">corepack</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> enable</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pnpm</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Clone ADH</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">git</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> clone</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> https://github.com/wan-h/awesome-digital-human-live2d.git</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> awesome-digital-human-live2d/web</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Install dependencies</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pnpm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Update frontend configs</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># NEXT_PUBLIC_SERVER_IP=&quot;&lt;ADH backend server IP&gt;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># NEXT_PUBLIC_SERVER_PORT=&quot;&lt;ADH backend server port&gt;&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">copy</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> .env_template</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> .env</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Run the ADH frontend server</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pnpm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> build</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">pnpm</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> run</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> start</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2)),s(r,null,{default:a(()=>e[26]||(e[26]=[t("测试 ADH 2D 实时数字人:")])),_:1,__:[26]}),e[44]||(e[44]=l('<p>在 UI 设置界面配置 AI Agent，我选择的是阿里云百炼模型广场的 <code>qwen3-8b</code>。</p><figure><img src="'+g+'" alt="ADH UI" tabindex="0" loading="lazy"><figcaption>ADH UI</figcaption></figure><h3 id="使用-agent-sdk-开发-ai-agent-对接-adh" tabindex="-1"><a class="header-anchor" href="#使用-agent-sdk-开发-ai-agent-对接-adh"><span>使用 Agent SDK 开发 AI Agent 对接 ADH</span></a></h3><p><a href="https://openai.github.io/openai-agents-python/" target="_blank" rel="noopener noreferrer">Agent SDK</a> 是 OpenAI 在 <a href="https://openai.com/index/new-tools-for-building-agents/" target="_blank" rel="noopener noreferrer">2025 年 3 月 11 号</a>发布的轻量级 AI Agent 开发框架。<a href="https://www.tizi365.com/openai-agents-sdk/" target="_blank" rel="noopener noreferrer">OpenAI Agents SDK 中文文档</a></p><blockquote><p><a href="https://github.com/openai/openai-cs-agents-demo" target="_blank" rel="noopener noreferrer">openai-cs-agents-demo</a> 是一个基于 OpenAI Agents SDK 构建的客户服务智能体演示项目，包含 Python 后端智能体编排引擎和 Next.js 前端交互界面。项目完整复现了航空公司客服场景：通过分流智能体（Triage Agent）将用户请求（如改签座位、航班状态查询）自动路由到专业智能体（座位预订/航班状态/FAQ 等模块），并集成了安全护栏机制（防越狱/防无关问题）。用户可通过直观的聊天界面体验多智能体协同处理复杂工作流的全过程，后端采用模块化设计便于自定义提示词和业务逻辑扩展。</p></blockquote><p>Agent SDK 中包括以下三个基本概念：</p>',6)),i("ul",null,[i("li",null,[s(n,null,{default:a(()=>e[27]||(e[27]=[t("Agent（代理）：配备指令和工具的一个 LLM 会话（session）。")])),_:1,__:[27]})]),i("li",null,[s(n,null,{default:a(()=>e[28]||(e[28]=[t("Handoff（移交）：允许一个代理委托其他代理来执行特定任务(多 Agent 协作)。")])),_:1,__:[28]})]),i("li",null,[s(n,null,{default:a(()=>e[29]||(e[29]=[t("Guardrail（护栏）：对代理的输入、输出进行校验。")])),_:1,__:[29]})])]),e[45]||(e[45]=l(`<p>在 Agent SDK 的实现中，是基于 Assistants API 来调用配置的 Tools。因此如果要调用 Tools，就必须使用支持 Assistants API（也就是所谓的“Function Call”）的 LLM。Qwen3 对 Assistants API 支持的非常好，而 DeepSeek-R1 则完全不支持 Assistants API，支持 Assistants API 的是 DeepSeek-V3。所以 DeepSeek-V3 和 DeepSeek-R1 必须结合起来才能开发调用 Tools 的 AI Agent。</p><p>多 Agent 协作的基础就是角色扮演（role playing），角色扮演的基础是大模型，现在主流的大模型都支持角色扮演。</p><p>回到 ADH 项目，在后端服务器上面创建一个新的 Python 项目，该项目作为一个 Python 库，添加为 ADH 的依赖。这是一种最简单的实现方式，还可以使用 RESTful API 调用的方式。</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> init</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> --python</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3.10</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> adh-ai-agent</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> adh-ai-agent</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Add agent sdk as a dependency</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> add</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> openai-agents</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># This is the name of the Python package</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">mkdir</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> adh_ai_agent</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>更新 <code>pyproject.toml</code> 文件，使用 <code>setuptools</code> 作为构建系统，以便于通过 pip 安装。</p><div class="language-toml line-numbers-mode" data-highlighter="shiki" data-ext="toml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[[</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">tool</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">index</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]]</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">url</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot;</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">default</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">true</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">build-system</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">requires</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = [</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;setuptools&gt;=42&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">build-backend</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;setuptools.build_meta&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>切换到 ADH 项目的根目录，更新 ADH 项目的 <code>pyproject.toml</code> 文件，添加 <code>editable_mode</code> 配置，以便于在开发过程中，自动更新依赖的包。</p><div class="language-toml line-numbers-mode" data-highlighter="shiki" data-ext="toml" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">[</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">tool</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">config-settings</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = { </span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">editable_mode</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;compat&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> }</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>添加 <code>adh-ai-agent</code> 作为 ADH 的依赖。</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">cd</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ~/awesome-digital-human-live2d</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">uv</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> pip</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> install</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -e</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;../adh-ai-agent&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>到这里，一个 AI Agent 项目的初始化工作就完成了。接下来开发一个 ADH agent 来支持调用上一步创建的外部 AI Agent。<code>digitalHuman/agent/core/repeaterAgent.py</code> 是一个最简单的 ADH Agent 实现，它重复输出用户输入的内容，可以作为模板开发一个新的 ADH Agent <code>digitalHuman/agent/core/externalAgent.py</code>。</p><blockquote><p>在 <code>run()</code> 函数中，使用 <code>importlib</code> 动态加载外部 AI Agent 的模块，调用了模块的 <code>chat_with_agent()</code> 函数。</p><p>还需要把新创建的 ADH Agent 添加到 <code>digitalHuman/agent/core/__init__.py</code> 文件中。然后在 <code>configs/agents/</code> 目录下创建一个新文件 <code>externalAgent.yaml</code>，添加新 ADH Agent 的配置。</p><p>最后，在主配置文件 <code>configs/config.yaml</code> 中，把新的 ADH Agent 配置文件名 <code>outsideAgent.yaml</code> 添加到 <code>SERVER.AGENTS.SUPPORT_LIST</code> 列表中。</p></blockquote><p>实现外部 AI Agent。</p><h3 id="数字人形象定制" tabindex="-1"><a class="header-anchor" href="#数字人形象定制"><span>数字人形象定制</span></a></h3><p>ADH 前端使用的 2D 数字人，是基于 Live2D 开发的。基于 Live2D 的 2D 数字人模型，可以划分为设计（也就是建模）和应用两个大的环节。</p>`,15)),i("ul",null,[i("li",null,[s(n,null,{default:a(()=>e[30]||(e[30]=[t("设计环节")])),_:1,__:[30]}),e[31]||(e[31]=t("：使用 Live2D Editor 来进行 2D 数字人的设计。设计的成品会被导出为 Live2D 模型。设计环节的主要工作由设计师来完成。"))]),i("li",null,[s(n,null,{default:a(()=>e[32]||(e[32]=[t("应用环节")])),_:1,__:[32]}),e[33]||(e[33]=t("：使用 Live2D SDK 将设计环节中导出的 Live2D 模型部署到各种应用之中。应用环节的主要工作由开发者来完成。"))])]),e[46]||(e[46]=i("p",null,[i("a",{href:"https://docs.live2d.com/zh-CHS/cubism-sdk-tutorials/top/",target:"_blank",rel:"noopener noreferrer"},"Live2D SDK 教程"),t("。ADH 前端是一个 Web 应用，选择的 SDK 是 "),i("a",{href:"https://docs.live2d.com/zh-CHS/cubism-sdk-tutorials/sample-build-web/",target:"_blank",rel:"noopener noreferrer"},"SDK for Web"),t("。")],-1)),s(r,null,{default:a(()=>e[34]||(e[34]=[t("数字人设计")])),_:1,__:[34]}),e[47]||(e[47]=l('<ol><li>下载和安装 <a href="https://www.live2d.com/zh-CHS/cubism/download/editor/" target="_blank" rel="noopener noreferrer">Live2D Editor</a>。安装好之后，其中有一个工具叫 Live2D Viewer，可以使用这个工具来查看和测试 Live2D Editor 导出的数字人模型。</li><li><a href="https://docs.live2d.com/zh-CHS/cubism-editor-tutorials/top/" target="_blank" rel="noopener noreferrer">Live2D Cubism 教程</a>, <a href="https://docs.live2d.com/zh-CHS/cubism-editor-manual/top/" target="_blank" rel="noopener noreferrer">Live2D Cubism 手册</a>。Live2D 数字人的设计环节，细节非常多，数字人要支持的姿态和动作越多，工作量就越大，也可以考虑去找人购买定制 Live2D 数字人的服务，或者在网上购买现成的 Live2D 数字人模型。<a href="https://www.live2d.com/zh-CHS/learn/sample/" target="_blank" rel="noopener noreferrer">Live2D 官网免费下载使用的模型</a>。此外，B 站就是中国 Live2D 设计师 + 开发者的大本营。在 B 站上，也可以看到很多分享的 Live2D 视频教程和 Live2D 模型。有些分享的模型是免费的，有些是收费的。</li><li>Live2D 数字人模型中包括以下文件(以 <code>{roleName}</code> 表示数字人角色名): <ul><li><code>{roleName}.model3.json</code>: 模型的配置文件</li><li><code>{roleName}.moc3</code>: 模型的骨骼文件</li><li><code>{roleName}.physics3.json</code>: 物理配置文件</li><li><code>{roleName}.cdi3.json</code>: 自定义参数数据</li><li><code>{roleName}.pose3.json</code>: 姿态配置文件</li><li><code>{roleName}.2048</code>: 这个子目录中是模型的纹理文件(通常是 png 图片)</li><li><code>motions</code>: 这个子目录中是各种动作的配置文件，后缀为 <code>.motion3.json</code></li><li><code>expressions</code>: 这个子目录中是各种表情的配置文件，后缀为 <code>.exp3.json</code></li></ul></li></ol>',1)),s(r,null,{default:a(()=>e[35]||(e[35]=[t("数字人应用")])),_:1,__:[35]}),e[48]||(e[48]=l(`<blockquote><p>以下所有操作是在 ADH 项目根目录下的 <code>web</code> 目录中进行。</p></blockquote><ol><li><p><a href="https://www.live2d.com/zh-CHS/sdk/about/" target="_blank" rel="noopener noreferrer">Live2D SDK 下载页面</a>选择 <code>SDK for Web</code> 下载。ADH 前端已经集成了 Live2D SDK for Web。</p></li><li><p><code>app/layout.tsx</code> 文件中的 script 标签中，添加 Live2D SDK 的引用（核心 JS 库）。</p></li><li><p><code>public/sentio</code> 目录是 Live2D 数字人模型的相关文件:</p><ul><li><code>characters</code>: 所有的 Live2D 数字人模型的相关文件</li><li><code>backgrounds</code>: 数字人的背景图片</li><li><code>core</code>: 只有一个 <code>live2dcubismcore.min.js</code> 文件，也就是 Live2D SDK 的核心 JS 库</li></ul></li><li><p><code>lib/live2d</code> 目录是 Live2D 相关的库，入口是 <code>live2dManager.js</code>:</p><ul><li><code>changeCharacter</code>: 设置不同的数字人模型</li><li><code>setLipFactor</code>, <code>getLipFactor</code>: 设置数字人的张口幅度参数</li><li>与语音相关的函数: <code>pushAudioQueue</code>, <code>popAudioQueue</code>, <code>clearAudioQueue</code>, <code>playAudio</code>, <code>stopAudio</code>, <code>isAudioPlaying</code></li></ul></li><li><p>在 ADH 前端配置新的数字人模型</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">mkdir</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -p</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> public/sentio/characters/custom/{roleName}</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># copy the Live2D model files to the custom directory</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>在 <code>lib/constants.ts</code> 文件中的 <code>SENTIO_CHARACTER_CUSTOM_MODELS</code> 列表中，添加新的数字人模型名。</p></li><li><p>语音个性化设置，也就是在 ADH 前端 TTS(语音合成) 的设置中，配置新的数字人模型的语音。目前 ADH 前端支持的 TTS 服务有 EdgeTTS, Dify 和<a href="https://cloud.tencent.com/product/tts" target="_blank" rel="noopener noreferrer">腾讯云</a>(<a href="https://console.cloud.tencent.com/cam/capi" target="_blank" rel="noopener noreferrer">API 密钥管理</a>)。</p></li></ol>`,2))])}const v=o(c,[["render",y]]),F=JSON.parse('{"path":"/categories/AI/Digital-Human/Real-time-2D.html","title":"Real-time 2D","lang":"en-US","frontmatter":{"title":"Real-time 2D","date":"2025-06-21T18:30:00.000Z","category":["ai","digital-human"],"tag":["ai","digital-human","real-time-2d","live 2D","adh","ai agent"],"description":"LLM 开源社区现有状态，非常活跃，大家开发 AI 原生应用时，可以参考选择。请以最新数据为准。 带着兴趣和疑问来听这个分享。分享完后，期待能够动手实践。 开源 LM 国内 阿里云 ，多模态 Qwen 2.5 VL, Qwen2.5-Omni , DeepSeek-V3, 多模态目前还未有 智谱 AI GLM-4，多模态 VisualGLM-6B, C...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Real-time 2D\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-21T18:30:00.000Z\\",\\"dateModified\\":\\"2025-06-28T11:27:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"DevOpsMe\\",\\"url\\":\\"https://github.com/JoneyXiao/devopsme\\"}]}"],["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/devopsme/categories/AI/Digital-Human/Real-time-2D.html"}],["meta",{"property":"og:site_name","content":"DevOpsMe"}],["meta",{"property":"og:title","content":"Real-time 2D"}],["meta",{"property":"og:description","content":"LLM 开源社区现有状态，非常活跃，大家开发 AI 原生应用时，可以参考选择。请以最新数据为准。 带着兴趣和疑问来听这个分享。分享完后，期待能够动手实践。 开源 LM 国内 阿里云 ，多模态 Qwen 2.5 VL, Qwen2.5-Omni , DeepSeek-V3, 多模态目前还未有 智谱 AI GLM-4，多模态 VisualGLM-6B, C..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-06-28T11:27:45.000Z"}],["meta",{"property":"article:tag","content":"ai agent"}],["meta",{"property":"article:tag","content":"adh"}],["meta",{"property":"article:tag","content":"live 2D"}],["meta",{"property":"article:tag","content":"real-time-2d"}],["meta",{"property":"article:tag","content":"digital-human"}],["meta",{"property":"article:tag","content":"ai"}],["meta",{"property":"article:published_time","content":"2025-06-21T18:30:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-28T11:27:45.000Z"}]]},"git":{"createdTime":1750583311000,"updatedTime":1751110065000,"contributors":[{"name":"Joney Xiao","username":"","email":"87732444@qq.com","commits":3}]},"readingTime":{"minutes":9.91,"words":2973},"filePathRelative":"categories/AI/Digital-Human/Real-time-2D.md","autoDesc":true}');export{v as comp,F as data};
