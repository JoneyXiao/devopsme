import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a as n,o as i}from"./app-CmV1Nd3J.js";const s="/devopsme/assets/data-pipeline-CDuPsv3f.png",r="/devopsme/assets/s1-BhpOvBK4.png",o="/devopsme/assets/c1-DdLLctMn.png",l="/devopsme/assets/c3-Zd4dSeJP.png",c="/devopsme/assets/victoria-arch-C_uhBYvB.png",d="/devopsme/assets/license-terms-BpWS04nl.png",p={};function u(h,e){return i(),t("div",null,e[0]||(e[0]=[n('<h1 id="splunk-data-pipeline" tabindex="-1"><a class="header-anchor" href="#splunk-data-pipeline"><span><a href="https://docs.splunk.com/Documentation/Splunk/latest/Deploy/Datapipeline" target="_blank" rel="noopener noreferrer">Splunk Data Pipeline</a></span></a></h1><figure><img src="'+s+'" alt="Splunk Data Pipeline" tabindex="0" loading="lazy"><figcaption>Splunk Data Pipeline</figcaption></figure><h2 id="input" tabindex="-1"><a class="header-anchor" href="#input"><span>Input</span></a></h2><ul><li>collects and consumes data from a variety of sources</li><li>the sources can be forwarded data, uploaded data, network data, scripts, api ...</li><li>splunk doesn&#39;t look at the contents of the data, it just passes it to the parsing phase</li></ul><h2 id="parsing" tabindex="-1"><a class="header-anchor" href="#parsing"><span>Parsing</span></a></h2><ul><li>happens on the indexer or heavy forwarder</li><li>examines, analyzes and transforms the data, identifies timestamps in the data, and annotates the data with metadata</li></ul><h2 id="indexing" tabindex="-1"><a class="header-anchor" href="#indexing"><span>Indexing</span></a></h2><ul><li>Writes the data to indexes on disk in the form of flat files stored in buckets which are just directories on the filesystem</li><li>Data divided into events. Writes the data to the disk in &quot;buckets&quot;</li></ul><h2 id="search" tabindex="-1"><a class="header-anchor" href="#search"><span>Search</span></a></h2><ul><li>User ineteraction with the data, including searching, building reports, alerting, graphing, and dashboarding</li></ul><h1 id="splunk-platform" tabindex="-1"><a class="header-anchor" href="#splunk-platform"><span>Splunk Platform</span></a></h1><p>Keep in mind, though, that Splunk has other products for IT Service Intelligence, SOAR (Security Orchestration, Automation, and Response), and more.</p><h2 id="splunk-enterprise" tabindex="-1"><a class="header-anchor" href="#splunk-enterprise"><span>Splunk Enterprise</span></a></h2><p>With Splunk Enterprise, you either install Splunk on-premises or datacenter in your facility, or in the cloud without specificlly using Splunk Cloud. This works very well for organizations that have both on-premises and cloud based systems, as well as for multi-cloud solutions. A Splunk Enterprise deployment is made up of Splunk components.</p><p><strong>Splunk Components: Installations of Splunk Enterprise that are configured to perform specific actions on the Splunk Data Pipeline.</strong></p><ul><li>Search Head: The main interface for searching and analyzing data.</li><li>Indexer: Indexes and stores data on disk.</li><li>Forwarder: Forwards data from source systems to either indexers or search heads.</li></ul><p>In a small deployment of Splunk, the search head and indexer can be the same component.</p><p><strong>Two main types of forwarders:</strong></p><ul><li>Universal Forwarder: a very lightweight agent, sits on a data source and forwards data</li><li>Heavy Forwarder: a more powerful agent, sits on a data source and forwards data, and also indexes data</li></ul><h3 id="splunk-architecture" tabindex="-1"><a class="header-anchor" href="#splunk-architecture"><span>Splunk Architecture</span></a></h3><h4 id="s1-s11-architecture-small-deployment" tabindex="-1"><a class="header-anchor" href="#s1-s11-architecture-small-deployment"><span><a href="https://docs.splunk.com/Documentation/SVA/current/Architectures/S1" target="_blank" rel="noopener noreferrer">S1/S11 Architecture</a>(small deployment)</span></a></h4><figure><img src="'+r+'" alt="Splunk S1/S11 Architecture" tabindex="0" loading="lazy"><figcaption>Splunk S1/S11 Architecture</figcaption></figure><ul><li>Search Head and Indexer are the same Splunk component</li><li>Daily data ingest up to 500GB</li><li>Small number of users</li><li>recommends S1 for non-critical data, there is no data replication or any kind of fault tolerance or load balancing</li></ul><p>s1(single server deployment) with three tiers:</p><ul><li>Searching and Indexing tier: search head / indexer combination</li><li>Collection tier: the data gets collected and forwarded</li><li>Management tier</li></ul><h4 id="c1-c11-architecture-distributed-clustered-deployment-single-site" tabindex="-1"><a class="header-anchor" href="#c1-c11-architecture-distributed-clustered-deployment-single-site"><span><a href="https://docs.splunk.com/Documentation/SVA/current/Architectures/C1C11" target="_blank" rel="noopener noreferrer">C1/C11 Architecture</a>(Distributed Clustered Deployment - Single Site)</span></a></h4><figure><img src="'+o+'" alt="Splunk C1/C11 Architecture" tabindex="0" loading="lazy"><figcaption>Splunk C1/C11 Architecture</figcaption></figure><ul><li>decouple the search head and indexer</li><li>One or more stand-alone(not clustered) search heads</li><li>an indexer cluster with data replication, and an indexer cluster manager which manages data replication among all the indexers in a cluster</li><li>multiple, load balanced collection inputs</li><li>good for disaster recovery and search speed</li></ul><p>The distributed clustered architecture environment for a single site.</p><ul><li>search tier</li><li>indexing tier</li><li>collection tier</li><li>management tier</li></ul><h4 id="c3-c13-architecture-distributed-clustered-deployment-with-search-head-clustering-single-site" tabindex="-1"><a class="header-anchor" href="#c3-c13-architecture-distributed-clustered-deployment-with-search-head-clustering-single-site"><span><a href="https://docs.splunk.com/Documentation/SVA/current/Architectures/C3C13" target="_blank" rel="noopener noreferrer">C3/C13 Architecture</a>(Distributed Clustered Deployment with search head clustering - Single-Site)</span></a></h4><figure><img src="'+l+'" alt="Splunk C3/C13 Architecture" tabindex="0" loading="lazy"><figcaption>Splunk C3/C13 Architecture</figcaption></figure><p>It&#39;s the same basic architecture as C1/C11, but with the addition of search head clustering.</p><ul><li>Search head cluster, and a <strong>deployer</strong> governs the members of the search head cluster which distributes apps, files and configuration updates to all of the members of the cluster through a configuration bundle</li><li>one of the members of the cluster is assigned to be the search head cluster <strong>captain</strong>, it schedules jobs and replication activities among the cluster</li><li>others are the same as in C1/C11</li></ul><p>More Splunk Validated Architectures can be found <a href="https://docs.splunk.com/Documentation/SVA/current/Architectures/About" target="_blank" rel="noopener noreferrer">here</a>.</p><h2 id="splunk-cloud-platform" tabindex="-1"><a class="header-anchor" href="#splunk-cloud-platform"><span><a href="https://docs.splunk.com/Documentation/SVA/current/Architectures/SCPExperience" target="_blank" rel="noopener noreferrer">Splunk Cloud Platform</a></span></a></h2><figure><img src="'+c+'" alt="Splunk Cloud Platform with Victoria Experience" tabindex="0" loading="lazy"><figcaption>Splunk Cloud Platform with Victoria Experience</figcaption></figure><p>Splunk Cloud Platform is a cloud-based service where Splunk manages the infrastructure, providing most of the benefits of a Splunk Enterprise deployment, with a few key differences(collection tier).</p><p>There are two cloud experiences:</p><ul><li>Classic</li><li>Victoria: new customers will be on Victoria which is hosted on AWS</li></ul><p>Splunk cloud is licensed through subscription types(subscription based):</p><ul><li>workload-based subscription is the default, it&#39;s based on resource capacity used, not data volume ingested, doesn&#39;t meter ingestion, can purchase additional capacity to increase performance, can purchase units of storage blocks for different data retention needs</li><li>ingest-based subscription, It requires an exception, similar to the Splunk Enterprise license, it&#39;s based on the volume of uncompressed data to index on a daily basis, it includes a fixed amount of data storage and can purchase additional storage as needed</li></ul><h3 id="apps-in-splunk-cloud" tabindex="-1"><a class="header-anchor" href="#apps-in-splunk-cloud"><span><a href="https://splunkbase.splunk.com/" target="_blank" rel="noopener noreferrer">Apps in Splunk Cloud</a></span></a></h3><ul><li>Only vetted and compatible apps</li><li>Some apps can be self installed through the app browser; others require a support ticket to be submitted</li><li>Private apps are supported, but are vetted by Splunk</li></ul><p>You do not have access to the Command Line Interface (CLI) or the underlying infrastructure. This means you will not be able to edit configuration files (.conf) or perform any CLI functions without submitting a support ticket to Splunk.</p><p>Direct data ingestion, such as TCP or syslog, is not allowed. However, you can still ingest this data; you just need to use a forwarder.</p><h1 id="how-splunk-stores-data" tabindex="-1"><a class="header-anchor" href="#how-splunk-stores-data"><span>How Splunk Stores Data</span></a></h1><h2 id="index" tabindex="-1"><a class="header-anchor" href="#index"><span>Index</span></a></h2><ul><li><p>Index is a repository for splunk data, when splunk processes raw incoming data, it adds that data to indexes, indexes map to places on the disk which calls buckets. Splunk has several built-in indexes, the <code>$SPLUNK_HOME/var/lib/splunk/defaultdb</code> is called main index, <code>_internal</code> index stores internal logs, we can create additional indexes as needed.</p></li><li><p>Splunk transforms incoming data into events, and stores it in indexes.</p></li><li><p>An event is just a single row of data with a bunch of key value pairs or fields.</p></li></ul><h2 id="event" tabindex="-1"><a class="header-anchor" href="#event"><span>Event</span></a></h2><ul><li>An event is a single row of data, made up of fields</li><li>Events have fields, which are key=value pairs. Splunk automatically looks for clear “key=value” entries in the data and creates fields. Splunk also looks for common key/value pairings that don’t necessarily have an equal sign</li><li>Splunk adds default fields to all events, the default fields are all required <ul><li><code>_time</code>: a timestamp value to events in Unix time regardless of how the time is already stored in the original data, it converts it to Unix time. If the data doesn&#39;t have time information, then splunk uses the time that the data was indexed for the time field.</li><li><code>index</code>: the index in which splunk is storing the data</li><li><code>host</code>: the hostname or ip address of the source system</li><li><code>source</code>: the name of the file, stream, or other input from which the event originates</li><li><code>sourcetype</code>: the format of the data, for example, a cisco syslog will come in as a source type <code>cisco_syslog</code>, a csv file will come in as a source type <code>csv</code>.</li></ul></li></ul><h2 id="bucket" tabindex="-1"><a class="header-anchor" href="#bucket"><span>Bucket</span></a></h2><p>An index contains compressed raw data and associated index files. These index files are spread out into different directories depending on their age, we can set their age in a configuration file. Splunk calls these directories buckets, there are five types of buckets in addition to a fishbucket:</p><ul><li>Hot bucket: newly indexed data, it&#39;s actively written to and searched, it&#39;s the data that&#39;s most available to a search head. After a certain amount of time, the data ages out into the warm bucket. An index has one or more hot buckets.</li><li>Warm bucket: The warm bucket has no active writing, then the data is aged and rolled to a cold bucket. An index has many warm buckets.</li><li>Cold bucket: Buckets rolled from warm and moved to a different location. An index has many cold buckets.</li><li>Frozen bucket: splunk deletes frozen buckets by default, but we can choose a different place to store them for archival purposes.</li><li>Thawed bucket: when restoring from a frozen bucket(archive), the data is thawed into a thawed bucket.</li></ul><p>The bucket ages can be set by modifying the configuration file <code>index.conf</code>.</p><p>The default directories of the buckets are:</p><ul><li>Hot bucket: <code>$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</code></li><li>Warm bucket: <code>$SPLUNK_HOME/var/lib/splunk/defaultdb/db/*</code></li><li>Cold bucket: <code>$SPLUNK_HOME/var/lib/splunk/defaultdb/colddb/*</code></li><li>Frozen bucket: <em>location that you specify for archival purposes</em></li><li>Thawed bucket: <code>$SPLUNK_HOME/var/lib/splunk/defaultdb/thaweddb/*</code></li></ul><h2 id="smartstore" tabindex="-1"><a class="header-anchor" href="#smartstore"><span>SmartStore</span></a></h2><p>As the data volume of a deployment increases, the demand for storage often exceeds the demand for compute resources. SmartStore enables you to manage your indexer storage and compute resources cost-effectively by allowing you to scale these resources independently.</p><ul><li>SmartStore allows you to use remote object stores like AWS S3, Azure Blob Storage, GCP Cloud Storage, etc</li><li>Most data resides on remote storage while the indexer maintains a local cache(hot buckets)</li></ul><h1 id="splunk-enterprise-licensing" tabindex="-1"><a class="header-anchor" href="#splunk-enterprise-licensing"><span>Splunk Enterprise Licensing</span></a></h1><p>Two types of enterprise Licensing <code>Volume-based</code> and <code>Infrastructure</code>.</p><h2 id="volume-based" tabindex="-1"><a class="header-anchor" href="#volume-based"><span>Volume-based</span></a></h2><ul><li>Based on your data indexed per day, not data stored</li><li>Daily indexing volume is measured from midnight to midnight by the clock on the license manager</li></ul><blockquote><p>Other things that do not count against your license</p><ul><li>Splunk internal log data</li><li>Duplicate data</li><li>Metadata</li></ul><p>License manager can be your search head, or a separate license server.</p><p>Licenses volume of 100GB per day or higher, if you exceed your licensed daily volume on any one calendar day, you get a warning, but search is not disabled</p><p>Licenses volume of less than 100GB per day, if you generate 45 license warnings in a 60 day period, you are in violation, and that license stack will not be searchable. You need to contact Splunk to reset that license.</p><p><a href="https://docs.splunk.com/Documentation/Splunk/9.4.1/Admin/Aboutlicenseviolations#What_happens_during_a_license_violation.3F" target="_blank" rel="noopener noreferrer">What happens during a license violation?</a></p></blockquote><h2 id="infrastructure" tabindex="-1"><a class="header-anchor" href="#infrastructure"><span>Infrastructure</span></a></h2><ul><li>Based on virtual CPUs(vCPU) <ul><li>Physical or logical core, virtual core, etc</li><li>Splunk uses the CPUs reported by the OS</li></ul></li><li>All search heads and indexers count towards vCPU capacity</li></ul><h2 id="which-components-need-a-license" tabindex="-1"><a class="header-anchor" href="#which-components-need-a-license"><span>Which components need a license?</span></a></h2><p>In a distributed environment, most Splunk Enterprise instances need access to an enterprise license (anything that indexes data). The exception is heavy forwarders, which only need a forwarder license, unless they are also indexing data, in which case they need enterprise licenses.</p><p>The ideal way is to set up a license master and have all of the Splunk instances in your environment talk to the master to get their licenses.</p><p>A collection of licenses whose individual licensing volume amounts aggregate to serve as a single unified amount of indexing volume is called a stack.</p><h2 id="license-pooling" tabindex="-1"><a class="header-anchor" href="#license-pooling"><span><a href="https://docs.splunk.com/Documentation/Splunk/9.4.1/Admin/Createalicensepool" target="_blank" rel="noopener noreferrer">License pooling</a></span></a></h2><figure><img src="'+d+`" alt="Splunk License Terms" tabindex="0" loading="lazy"><figcaption>Splunk License Terms</figcaption></figure><p><a href="https://docs.splunk.com/Documentation/Splunk/9.4.1/Admin/Groups,stacks,pools,andotherterminology" target="_blank" rel="noopener noreferrer">LICENSE GROUPS</a> are sets of license stacks. A stack can only be a member of one group, and only one group can be “active” at any given time. The license groups are:</p><ul><li>Enterprise/Sales Trial Group: Allows stacking of purchased enterprise licenses</li><li>Enterprise Trial Group: Default group. Cannot stack licenses</li><li>Free Group: No stacking</li><li>Forwarders Group: No stacking</li></ul><p>LICENSE POOLS</p><ul><li>License pools are created from license stacks</li><li>Pools are sized for specific purposes</li><li>Managed by the license manager</li><li>Indexers and other Splunk Enterprise components are assigned to a pool</li><li>Splunk recommends not assigning forwarders to a license pool, since they have unique license types</li></ul><h1 id="configuration-files" tabindex="-1"><a class="header-anchor" href="#configuration-files"><span>Configuration Files</span></a></h1><ul><li>Everything Splunk does is governed by configuration files</li><li>Configuration files are stored in <code>/etc</code>, and they have the <code>.conf</code> extension</li><li>Configuration files are layered <ul><li>You can have <code>.conf</code> files that have the same name in different directories</li><li>Splunk determines which one to use based on the current app</li><li>Use <code>btool</code> to determine which <code>.conf</code> file is being used</li></ul></li><li>The <code>/etc/&lt;app&gt;/default</code> directory contains preconfigured versions of <code>.conf</code> files</li><li>The <code>/etc/&lt;app&gt;/local</code> directory is where custom configurations are stored</li></ul><p>Configuration File Structure:</p><div class="language-conf line-numbers-mode" data-highlighter="shiki" data-ext="conf" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># text file</span></span>
<span class="line"><span>[Stanza]</span></span>
<span class="line"><span>Attribute = Value</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[Stanza]</span></span>
<span class="line"><span>Attribute = Value</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="splunk-apps-and-add-ons" tabindex="-1"><a class="header-anchor" href="#splunk-apps-and-add-ons"><span>Splunk Apps and Add-ons</span></a></h1><p>Apps extend Splunk’s functionality, they enhance the user experience with built in dashboards, reports, alerts, saved searches, data models, conf files, etc. An app is a collection of Splunk configuration files, views, knowledge objects, and sometimes add-ons.</p><p>Add-ons primarily support apps, they enrich the incoming data with tags, data models, datasets, etc. An add-on is a subset of an app and specifies data collection, but doesn&#39;t usually have visualizations because they are part of the larger app.</p><ul><li>Apps and add-ons can be created by individuals, third party companies, Splunk, or vendors.</li><li>Apps and add-ons that are built by Splunk carry the ”Splunk Built” logo</li><li>Apps and add-ons that are certified by Splunk carry the “Splunk Certified” logo</li></ul>`,85)]))}const g=a(p,[["render",u]]),k=JSON.parse('{"path":"/categories/splunk/Splunk-Architecture.html","title":"Splunk Architecture","lang":"en-US","frontmatter":{"title":"Splunk Architecture","date":"2025-04-03T17:17:00.000Z","categories":["splunk"],"tags":["splunk","splunk-architecture","splunk-deployment-architecture"],"description":"Splunk Data Pipeline Splunk Data PipelineSplunk Data Pipeline Input collects and consumes data from a variety of sources the sources can be forwarded data, uploaded data, networ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Splunk Architecture\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-04-03T17:17:00.000Z\\",\\"dateModified\\":\\"2025-05-28T15:10:02.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"DevOpsMe\\",\\"url\\":\\"https://github.com/JoneyXiao/devopsme\\"}]}"],["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/devopsme/categories/splunk/Splunk-Architecture.html"}],["meta",{"property":"og:site_name","content":"DevOpsMe"}],["meta",{"property":"og:title","content":"Splunk Architecture"}],["meta",{"property":"og:description","content":"Splunk Data Pipeline Splunk Data PipelineSplunk Data Pipeline Input collects and consumes data from a variety of sources the sources can be forwarded data, uploaded data, networ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-05-28T15:10:02.000Z"}],["meta",{"property":"article:tag","content":"splunk-deployment-architecture"}],["meta",{"property":"article:tag","content":"splunk-architecture"}],["meta",{"property":"article:tag","content":"splunk"}],["meta",{"property":"article:published_time","content":"2025-04-03T17:17:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-28T15:10:02.000Z"}]]},"git":{"createdTime":1744280103000,"updatedTime":1748445002000,"contributors":[{"name":"Joney Xiao","username":"","email":"87732444@qq.com","commits":6}]},"readingTime":{"minutes":6.75,"words":2026},"filePathRelative":"categories/splunk/Splunk-Architecture.md","autoDesc":true}');export{g as comp,k as data};
